---
#draft: true
title: Tools & Libraries
summary:  Tools & software libraries for data-driven decision-making problems.
tags:
  - Libs
date: '2023-10-01T00:00:00Z'

# Optional external URL for project (replaces project detail page).
external_link: ''

image:
  caption: Generated by Microsoft Designer
  focal_point: Smart
  preview_only: false

# links:
#   - icon: twitter
#     icon_pack: fab
#     name: Follow
#     url: https://twitter.com/georgecushen
# url_code: ''
# url_pdf: ''
# url_slides: ''
# url_video: ''

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example
---

<div style="font-family: Helvetica, sans-serif; max-width: 960px; margin: 0 auto; padding: 20px; line-height: 1.6; color: #333;">

  <div style="
    padding: 2px;
    border-radius: 12px;
    background: linear-gradient(135deg, #e0f2fe, #ecfdf5);
    box-shadow: 0 4px 12px rgba(0,0,0,0.05);
">
    <div style="
        background: white;
        border-radius: 10px;
        padding: 20px;
    ">
        <p style="
            font-size: 18px;
            line-height: 1.7;
            color: #1e293b;
            margin: 0;
        ">
            We provide open code implementations for most of our research, please check our papers for related codes. In addition, we aim to develop easy-to-use and comprehensive algorithm libraries and tools to accelerate the real-world deployment of advanced data-driven decision-making methods.
        </p>
    </div>
</div>

<h3 style="margin-top: 24px; color: #00bcd4; font-size: 24px; text-align: center;">Data-Drivien Decision-Making Libraries / Tools</h3>

![screen reader text](d2c-logo.png "")


<div style="font-family: Helvetica, sans-serif; max-width: 960px; margin: 0 auto; padding: 20px; line-height: 1.6; color: #333;">

  <div style="
    padding: 2px;
    border-radius: 12px;
    background: linear-gradient(135deg, #e0f2fe, #ecfdf5);
    box-shadow: 0 4px 12px rgba(0,0,0,0.05);
">
    <div style="
        background: white;
        border-radius: 10px;
        padding: 20px;
    ">
        <p style="
            font-size: 18px;
            line-height: 1.7;
            color: #1e293b;
            margin: 0;
        ">
            <a href="https://github.com/AIR-DI/D2C">Data-Driven Control Lib (D2C)</a> is a library for data-driven decision-making & control based on state-of-the-art offline reinforcement learning (RL), offline imitation learning (IL), and offline planning algorithms. It is a platform for solving various decision-making & control problems in real-world scenarios. D2C is designed to offer fast and convenient algorithm performance development and testing, as well as providing easy-to-use toolchains to accelerate the real-world deployment of SOTA data-driven decision-making methods.
        </p>
    </div>
</div>

<h3 style="margin-top: 24px; color:rgb(94, 120, 225); font-size: 20px;">The current supported offline RL/IL algorithms include (more to come):</h3>

- [Twin Delayed DDPG with Behavior Cloning (TD3+BC)](https://arxiv.org/pdf/2106.06860.pdf)
- [Distance-Sensitive Offline Reinforcement Learning (DOGE)](https://arxiv.org/abs/2205.11027.pdf)
- [Dynamics-Aware Hybrid Offline-and-Online Reinforcement Learning (H2O)](https://arxiv.org/abs/2206.13464.pdf)
- [Sparse Q-learning (SQL)](https://arxiv.org/abs/2303.15810)
- [Policy-guided Offline RL (POR)](https://arxiv.org/abs/2210.08323)
- [Offline Reinforcement Learning with Implicit Q-Learning (IQL)](https://arxiv.org/pdf/2110.06169.pdf)
- [Discriminator-Guided Model-Based Offline Imitation Learning (DMIL)](https://arxiv.org/abs/2207.00244)
- [Behavior Cloning (BC)](http://www.cse.unsw.edu.au/~claude/papers/MI15.pdf)

<h3 style="margin-top: 24px; color:rgb(94, 120, 225); font-size: 20px;">Features:</h3>

- D2C includes a large collection of offline RL and IL algorithms: model-free and model-based offline RL/IL algorithms, as well as planning methods. 
- D2C is highly modular and extensible. You can easily build custom algorithms and conduct experiments with it.
- D2C automates the development process in real-world control applications. It simplifies the steps of problem definition/mathematical formulation, policy training, policy evaluation and model deployment.

<h3 style="margin-top: 24px; color:rgb(94, 120, 225); font-size: 20px;">Library Information:</h3>

- The library is available in [https://github.com/AIR-DI/D2C](https://github.com/AIR-DI/D2C).
- The tutorials and API documentation are hosted on [air-d2c.readthedocs.io](https://air-d2c.readthedocs.io/).


<h3 style="margin-top: 24px; color: #00bcd4; font-size: 24px; text-align: center;">Online RL Library</h3>

<div style="font-family: Helvetica, sans-serif; max-width: 960px; margin: 0 auto; padding: 20px; line-height: 1.6; color: #333;">

  <div style="
    padding: 2px;
    border-radius: 12px;
    background: linear-gradient(135deg, #e0f2fe, #ecfdf5);
    box-shadow: 0 4px 12px rgba(0,0,0,0.05);
">
    <div style="
        background: white;
        border-radius: 10px;
        padding: 20px;
    ">
        <p style="
            font-size: 18px;
            line-height: 1.7;
            color: #1e293b;
            margin: 0;
        ">
            <a href="https://github.com/imoneoi/onerl">OneRL</a>: Event-driven fully distributed reinforcement learning framework proposed in <a href="https://arxiv.org/abs/2110.11573">"A Versatile and Efficient Reinforcement Learning Approach for Autonomous Driving"</a> that can facilitate highly efficient policy learning in RL-based tasks.
        </p>
    </div>
</div>

<h3 style="margin-top: 24px; color:rgb(94, 120, 225); font-size: 20px;">Features:</h3>

- Super fast RL training! (15~30min for MuJoCo & Atari on single machine)
- State-of-the-art performance
- Scheduled and pipelined sample collection
- Completely lock-free execution
- Fully distributed architecture
- Full profiling & overhead identification tools 
- Online visualization & rendering
- Support multi-GPU parallel training
- Support exporting trained policy to ONNX for faster inference & deployment

